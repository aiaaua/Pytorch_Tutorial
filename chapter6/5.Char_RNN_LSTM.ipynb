{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import time, math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-24 22:49:16--  https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "접속 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... 접속됨.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘./data/input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  2.89MB/s    in 0.4s    \n",
      "\n",
      "2021-02-24 22:49:17 (2.89 MB/s) - ‘./data/input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt -P ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#출력 가능한 모든 문자 불러옴\n",
    "all_characters = string.printable\n",
    "all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_characters = len(all_characters)\n",
    "n_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = unidecode.unidecode(open('./data/input.txt').read())\n",
    "file_len = len(file)\n",
    "file_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 100\n",
    "plot_every = 10\n",
    "\n",
    "chunk_len = 200\n",
    "\n",
    "num_epochs = 2000\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "num_layers = 1\n",
    "embedding_size = 70\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일의 일부분을 랜덤하게 불러오는 함수\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문자열을 인덱스 배열로 바꿔주는 함수\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for i in range(len(string)):\n",
    "        tensor[i] = all_characters.index(string[i])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random text chunk를 불러와서 입력과 목표값을 바꿔주는 함수\n",
    "def random_training_set():\n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    tar = char_tensor(chunk[1:])\n",
    "    return inp, tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.rnn = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers)\n",
    "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, inp, hidden, cell):\n",
    "        out = self.encoder(inp.view(1, -1))\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.decoder(out.view(batch_size, -1))\n",
    "        return out, hidden, cell\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size = n_characters, \n",
    "            embedding_size = embedding_size,\n",
    "            hidden_size = hidden_size,\n",
    "            output_size = n_characters, \n",
    "            num_layers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#임의의 문자로 시작하는 길이 200짜리 모방글 생성\n",
    "def test():\n",
    "    start_str = \"b\"\n",
    "    inp = char_tensor(start_str)\n",
    "    hidden, cell = model.init_hidden()\n",
    "    \n",
    "    print(start_str, end=\"\")\n",
    "    \n",
    "    for i in range(200):\n",
    "        output, hidden, cell = model(inp, hidden, cell)\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_n = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_characters[top_n]\n",
    "        print(predicted_char, end=\"\")\n",
    "        inp = char_tensor(predicted_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([4.5646], grad_fn=<DivBackward0>) \n",
      "\n",
      "bS\n",
      "Rh\\S@tz-)Tz;uwYTC+@/>r|N!4ZaI;/E22.\u000b",
      "B2\n",
      "br\"aC6V\tn59+R*5bD#6l!:Pj]66!^[,mWV7ky@|zz9Lmx!`!Nhp{{3@EP\u000b",
      ")pYw#S:Y\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([3.0038], grad_fn=<DivBackward0>) \n",
      "\n",
      "bDbd tuhe mth yntmns. womans wes mthuer h ttere ns sioe froteiaoemeh stir\n",
      "et stti'oaihnh aran aires ohee\n",
      "tirhe mr  he tdad eIot urer uitou yre lAap nosisenudh nes satrpuinh ss laoiae wrosnser whawl hor\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.4678], grad_fn=<DivBackward0>) \n",
      "\n",
      "buk hou, oun mod tou lune oh med hof ad dau be this,\n",
      "Ar ie thon gow noce on be ae sepanle thou 'oin rol ci the goor!\n",
      "\n",
      "R:\n",
      "'en yheve drasan; itnraen saur tine gan thiun das peirecI ly ged Cod wou nad dee\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.3190], grad_fn=<DivBackward0>) \n",
      "\n",
      "be wig, mee,\n",
      "The, the moull,\n",
      "Iles on shos won mimesind me the hild cot min ons't all, feod Goit thain weo hom tar on int borgol mosgoy mor menk id hict onle fonu\n",
      "Thit yony mith ens ytimee pirto rererel\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.5541], grad_fn=<DivBackward0>) \n",
      "\n",
      "bd endud mio errer cilt shar has eregilnn,\n",
      "The or I thas barfottet in mer;\n",
      "And bethe 'the Carkellen in, me ordine nto hakr ar there bow avect poove speger,\n",
      "Bathe as asrathe, she movis nend und mund thl\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.3087], grad_fn=<DivBackward0>) \n",
      "\n",
      "bor hiler.\n",
      "\n",
      "BEOR ENI:\n",
      "Sou them mer sornd gid ball me worpece wivead a?\n",
      "Thamens be silt his woll wonnend whay I the miszaes kare,\n",
      "Wenbind dey wound angaw tinded hecor wave sof wipe.\n",
      "\n",
      "Whas mut kargo toe \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1294], grad_fn=<DivBackward0>) \n",
      "\n",
      "ball dice in thous my:\n",
      "And cits leriogh the ith,\n",
      "This socben ris soots caule the whard to the past beet and fifhes bean, no thir to hat? in, buk freargise\n",
      "And on as as cich'f healse fere Youd I the at-\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2220], grad_fn=<DivBackward0>) \n",
      "\n",
      "binsens hy soukrels if hich dirta thind  muth her fawhen thive lichs wartels'ls to are, his sous hid Lorn, thind and bestht he afes; the bask hry reesrer my I rath or thio and min?\n",
      "\n",
      "SIS:\n",
      "The hinds ono \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0218], grad_fn=<DivBackward0>) \n",
      "\n",
      "bk ige of my, fore wo thin the by shoun bow by hath the be uill the hee youy un he bares am of porlevime vove, sos mare what lanqthour my froringe and wish what, whad it the start are?\n",
      "\n",
      "GOSRUY:\n",
      "Nuret f\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2537], grad_fn=<DivBackward0>) \n",
      "\n",
      "b the ry dean, Whour hand;\n",
      "And and and hance and bime sound ther me gost'd in.\n",
      "Thou houne a thound for not the in of a the hort me wath the, she pore to wore is foue the a were colr and youz stams ih m\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1963], grad_fn=<DivBackward0>) \n",
      "\n",
      "b\\O noce\n",
      "Catare sor chedey sat wow head cout spaed, your onos there sum thith and, thy coverle haciesh the lound mards\n",
      "To do food Clall your have a westostey to preat.\n",
      "\n",
      "PLUETEL:\n",
      "Whas heand ase,\n",
      "When ni\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9125], grad_fn=<DivBackward0>) \n",
      "\n",
      "brest meseot for lirk.\n",
      "\n",
      "LETBONIO:\n",
      "I'\n",
      "Ake so kan thour sere and ham lomo, lome your I hast whas frowhirs, dore to lav, be a buche our lecy deverusursin and suan now he marreb's a that Lorss wroust by ir\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9868], grad_fn=<DivBackward0>) \n",
      "\n",
      "be vare the dike nayt of my domethless.\n",
      "\n",
      "GUETENTA:\n",
      "The kso-so daans the bea west,\n",
      "Mid noth of.\n",
      "\n",
      "TROUTARE BUKE:\n",
      "Sily, nok there to stish wist she provelsing plame if Dentostert.\n",
      "\n",
      "VOLMETVERTENM:\n",
      "He hacer\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1785], grad_fn=<DivBackward0>) \n",
      "\n",
      "ble?\n",
      "\n",
      "Arishery, sul and: ace thy there ar frave colle, the hade,\n",
      "Whid the it that my to to crestind be?\n",
      "\n",
      "DUUNDIUS:\n",
      "MI LOd Pive lordemt.\n",
      "\n",
      "MESWERMEMARW:\n",
      "Ferout our the ceirnkt, thace preatent nech in sai\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8878], grad_fn=<DivBackward0>) \n",
      "\n",
      "bed your wearn wear firghs you, a wipled on reed now\n",
      "Whers mase thing the lord, to ther lord bear fare lend gond there pret of sue stam's your beines, he cepends hiwing daal sell gor king mave Lyor hav\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8271], grad_fn=<DivBackward0>) \n",
      "\n",
      "bast sparting oufh in I haw mistall be! same an witharate the calr?\n",
      "\n",
      "Sethere largisce saed him make the kind.\n",
      "He ford net a dear good,\n",
      "Therurss you the wand marly her wordt here stang mards werlincmer \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0135], grad_fn=<DivBackward0>) \n",
      "\n",
      "bend tive for you dearnimine this of ho marrinst you heart han can brome stand be sporroof and thy lood all come.\n",
      "\n",
      "Tire the saming boobfer, ank or mare on in\n",
      "of that and must so sher but the knithers a\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8988], grad_fn=<DivBackward0>) \n",
      "\n",
      "bees, sceantent 'wer come?\n",
      "\n",
      "BROMONGO:\n",
      "What to the the dook!\n",
      "\n",
      "BRACIO:\n",
      "Whan a daiver the spear all mice this thould save you soses,\n",
      "And as\n",
      "Some and purst the we then sure Edcent, frow bente, for age in t\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8921], grad_fn=<DivBackward0>) \n",
      "\n",
      "but of perter and us to smand all the trutangom and and thee and pears.\n",
      "\n",
      "AN\n",
      "VALINGDA:\n",
      "As in shall be this bis,\n",
      "Is I the, if not the so chen surse, out ound; not but in the shourd,\n",
      "I sting ous noth grou\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9908], grad_fn=<DivBackward0>) \n",
      "\n",
      "bet me this devel and him thou speaming\n",
      "And air one of drom be this this inst my tilote the that the let the mall! the the hive the in event hers me vancent anther the dale thou bece that this tonket t\n",
      " ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    inp, label = random_training_set()\n",
    "    hidden, cell = model.init_hidden()\n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for j in range(chunk_len-1):\n",
    "        x = inp[j]\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
    "        y, hidden, cell = model(x, hidden, cell)\n",
    "        loss += loss_func(y, y_)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print(f'\\n {loss/chunk_len} \\n')\n",
    "        test()\n",
    "        print('\\n', '='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
