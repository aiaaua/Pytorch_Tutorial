{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import time, math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-24 10:20:57--  https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "접속 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... 접속됨.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘./data/input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M   896KB/s    in 1.2s    \n",
      "\n",
      "2021-02-24 10:20:59 (896 KB/s) - ‘./data/input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt -P ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#출력 가능한 모든 문자 불러옴\n",
    "all_characters = string.printable\n",
    "all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_characters = len(all_characters)\n",
    "n_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = unidecode.unidecode(open('./data/input.txt').read())\n",
    "file_len = len(file)\n",
    "file_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 100\n",
    "plot_every = 10\n",
    "\n",
    "chunk_len = 200\n",
    "\n",
    "num_epochs = 2000\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "num_layers = 1\n",
    "embedding_size = 70\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일의 일부분을 랜덤하게 불러오는 함수\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문자열을 인덱스 배열로 바꿔주는 함수\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for i in range(len(string)):\n",
    "        tensor[i] = all_characters.index(string[i])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random text chunk를 불러와서 입력과 목표값을 바꿔주는 함수\n",
    "def random_training_set():\n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    tar = char_tensor(chunk[1:])\n",
    "    return inp, tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.rnn = nn.GRU(self.embedding_size, self.hidden_size, self.num_layers)\n",
    "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, inp, hidden):\n",
    "        out = self.encoder(inp.view(1, -1))\n",
    "        out, hidden = self.rnn(out, hidden)\n",
    "        out = self.decoder(out.view(batch_size, -1))\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size = n_characters, \n",
    "            embedding_size = embedding_size,\n",
    "            hidden_size = hidden_size,\n",
    "            output_size = n_characters, \n",
    "            num_layers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#임의의 문자로 시작하는 길이 200짜리 모방글 생성\n",
    "def test():\n",
    "    start_str = \"b\"\n",
    "    inp = char_tensor(start_str)\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    print(start_str, end=\"\")\n",
    "    \n",
    "    for i in range(200):\n",
    "        output, hidden = model(inp, hidden)\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_n = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_characters[top_n]\n",
    "        print(predicted_char, end=\"\")\n",
    "        inp = char_tensor(predicted_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([4.5482], grad_fn=<DivBackward0>) \n",
      "\n",
      "bqPxEq]&!u'+M~x(IA\">oQ4\\\u000b",
      "45zT#EbZ.Kd%hAr)3(k]8R`e@%)xfnz:gS![<iu55i8U\n",
      "_\u000b",
      "i7V7n\"/doFsLMZ-8pxM}KYn0@uZt\\2gTL7IRTzh|eP98`8mvuK<\n",
      "r4#cHh1(CD\")@7QrnF<0% A9,uA'NC+JU`4<sR}R6\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.7238], grad_fn=<DivBackward0>) \n",
      "\n",
      "^OAS:\n",
      "ThTer, ce rone wpot thianee that ounc;e be de at cet wher.\n",
      "\n",
      "Aisupt touv wocher fime wit ces,.\n",
      "s pat t urs: ib merd alit met copursy roul thek nooit!\n",
      "Wohe safapojtaf ant sou pld hocir tout h\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2038], grad_fn=<DivBackward0>) \n",
      "\n",
      "bust\n",
      "wif ald ins why hat,, me faon my sid, beull stinou the hint wo wo ur in thou, 'tir;\n",
      "Whand nacit litfe, rarfincelt a it has werindt dot to shate's, Iill tharsar hor ad then a con merrefove sintithe\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2823], grad_fn=<DivBackward0>) \n",
      "\n",
      "biin shour rithas lill but yey is tis dell'ere of banty,\n",
      "On hat the maden men thee fonrere in breare elcuad mes the ned I don in be the hers me and Endou-\\e, and be tean the netang this kate here-vert \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0217], grad_fn=<DivBackward0>) \n",
      "\n",
      "bloy the the that comle but I stiadsen, nod theo unang; cand my to her.\n",
      "And my cown in wher one!\n",
      "In neme thue brleamO\n",
      "And wour,\n",
      "To her touk to mome the by wive to that sith ather.\n",
      "\n",
      "CELUCERERD Od I hust\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8454], grad_fn=<DivBackward0>) \n",
      "\n",
      "blad,\n",
      "This nows for sendence;\n",
      "I wist to thou, to hass conting reckure, serne this gall the the fpine thee maned the me; your gore the were\n",
      "For hat homes, the mo ferime shath the me thy the that lestreb\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1998], grad_fn=<DivBackward0>) \n",
      "\n",
      "bny in the pery to king o!\n",
      "Ntan that men sim.\n",
      "\n",
      "MORSIOIUK:\n",
      "And strout me ich all's he fery beorth ye toke pride:\n",
      "Lartian to the everss:\n",
      "To go deice,\n",
      "To arter, we gods siy, and becort not on ipiik the th\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8306], grad_fn=<DivBackward0>) \n",
      "\n",
      "bringer'd plomt and in forthat or thice, I dight thee, to being alittenk our they this pame quirgey undep? Pence firthese with paath them if how my lion'd that the meeman, and a the the my separlong ne\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2756], grad_fn=<DivBackward0>) \n",
      "\n",
      "boke of fropure my do shoul my so,\n",
      "Dire kind, but come true\n",
      "And come a she lords make.\n",
      "\n",
      "RIRHARHALUS:\n",
      "A chare the shall and must will be harpwere mest but seam,\n",
      "And lord,\n",
      "And the waight'g, CaSwies have \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8203], grad_fn=<DivBackward0>) \n",
      "\n",
      "b?\n",
      "A swere, on that Clastale,\n",
      "Uf revood of that on, an eavel the sperance's march amblest shall nemand crading bears.\n",
      "\n",
      "CPORENSAR ELIO:\n",
      "Rown,\n",
      "And pronce me man dich\n",
      "To that cupion: I ad he\n",
      "whate,\n",
      "Whould\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.6670], grad_fn=<DivBackward0>) \n",
      "\n",
      "bur's lether her the srebitives to Rrided our bends of this withen that, our her too dist words\n",
      "Tell and your so limfect busture woussing, her shalman:\n",
      "What but thou drerest with a prived preven be a s\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7233], grad_fn=<DivBackward0>) \n",
      "\n",
      "be that not the nervest a at lords?\n",
      "'That a do mean: here streath?\n",
      "\n",
      "EPHORDY:\n",
      "Will is dick Clantaded firrests!\n",
      "O gold man come hir day there's come how his stong firgh else we'll upoted me thas deace je\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8109], grad_fn=<DivBackward0>) \n",
      "\n",
      "be cued\n",
      "It Iing cose comprion for, son I have it feen wrons, thantsent\n",
      "On creor conseet it to he chide, mirle liki'll baster in thus sechal not markanty darech, time dood me swir's ba my ear vise pith.\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.6244], grad_fn=<DivBackward0>) \n",
      "\n",
      "bforcest an cace stir, I stas word,\n",
      "To ste'd of thorse,\n",
      "You not when were seritore of the swarbless.\n",
      "\n",
      "KAPNIUS:\n",
      "I to the stay.\n",
      "You licknang\n",
      "So the consijelst the pichand\n",
      "Whip aclricion it and adlest of \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9587], grad_fn=<DivBackward0>) \n",
      "\n",
      "bdow a shall be and as distain than but whee the your\n",
      "Horge bring.\n",
      "\n",
      "LUCIO:\n",
      "But rato, thou at a she blood, husire nhow ands as is whooe\n",
      "Hears my was pellone\n",
      "Thou is lame the lord be thy not\n",
      "To her the v\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7330], grad_fn=<DivBackward0>) \n",
      "\n",
      "by to wastain! you to you, head in her wor-do his gropent\n",
      "They frier gentally a which Have strengest\n",
      "Maighted now hath rust in would hois.\n",
      "\n",
      "CEFIRA:\n",
      "O done the countair, to cite our may myself trous,\n",
      "If\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7258], grad_fn=<DivBackward0>) \n",
      "\n",
      "be to to thy lead's made speet angring.\n",
      "\n",
      "BUCHENITI:\n",
      "From would becition, and my deed but Served:\n",
      "With hear thy but onatius perton,\n",
      "Yet bebes; had heir, duplent forth not prettle:\n",
      "He neable alonent?\n",
      "\n",
      "RI\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8003], grad_fn=<DivBackward0>) \n",
      "\n",
      "but of this days.\n",
      "Heef then deep will herene come pate ut steak the rison whod here a sire with the night\n",
      "To the hasselds her, ang see the seak the know-hewand seeff, seath.\n",
      "She are toed old shamper my\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8615], grad_fn=<DivBackward0>) \n",
      "\n",
      "bnusperstantoch his rick my sig: Kately her, us will sparky, resermand, that troess of sounce.\n",
      "\n",
      "CLERCUCIO:\n",
      "A will you the meay no post see\n",
      "Where all sharper, to the firs the lountay is his him;\n",
      "Both a \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7803], grad_fn=<DivBackward0>) \n",
      "\n",
      "bD Parcont.\n",
      "\n",
      "ROME:\n",
      "Sheak his from not wome to mest thou man deafort of thou corscen:\n",
      "\n",
      "Lord, but of siccing,\n",
      "Thish'd, whose you mest shall spome usty you so mother of gooded pleated one my sweet:\n",
      "Should\n",
      " ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    inp, label = random_training_set()\n",
    "    hidden = model.init_hidden()\n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for j in range(chunk_len-1):\n",
    "        x = inp[j]\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
    "        y, hidden = model(x, hidden)\n",
    "        loss += loss_func(y, y_)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print(f'\\n {loss/chunk_len} \\n')\n",
    "        test()\n",
    "        print('\\n', '='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
